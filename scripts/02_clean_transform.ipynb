{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb6617a4-369a-4ed0-ae4d-e95a59974fcd",
   "metadata": {},
   "source": [
    "# ============================================\n",
    "# 02_clean_transform.ipynb\n",
    "# Step 2: Data Cleaning and Transformation\n",
    "# ============================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0c8d838c-9f7e-45d2-b81d-d11c6ffaf234",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from datetime import timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0cada498-679e-4d50-843f-be957394c5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ 1. Define input/output paths\n",
    "RAW_DIR = \"../data_raw\"\n",
    "CLEAN_DIR = \"../data_clean\"\n",
    "os.makedirs(CLEAN_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2a7d8918-7095-4ae6-b45e-e4fc17de2e3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading raw data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\91790\\AppData\\Local\\Temp\\ipykernel_2588\\3320391407.py:19: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(os.path.join(RAW_DIR, file), dtype=dtypes)\n",
      "C:\\Users\\91790\\AppData\\Local\\Temp\\ipykernel_2588\\3320391407.py:19: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(os.path.join(RAW_DIR, file), dtype=dtypes)\n",
      "C:\\Users\\91790\\AppData\\Local\\Temp\\ipykernel_2588\\3320391407.py:19: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(os.path.join(RAW_DIR, file), dtype=dtypes)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 9,384,487 taxi trips from 3 files.\n",
      "Loaded 90 weather records.\n"
     ]
    }
   ],
   "source": [
    "# # -----------------------------\n",
    "# 1️⃣ Load Data\n",
    "# -----------------------------\n",
    "print(\"Loading raw data...\")\n",
    "\n",
    "# Load Taxi Data (combining 3 months)\n",
    "taxi_files = [f for f in os.listdir(RAW_DIR) if f.startswith('yellow_tripdata') and f.endswith('.csv')]\n",
    "taxi_dfs = []\n",
    "for file in taxi_files:\n",
    "    # Use selected dtypes to reduce memory usage and ensure correct parsing\n",
    "    dtypes = {\n",
    "        'VendorID': 'Int64', 'PULocationID': 'Int64', 'DOLocationID': 'Int64',\n",
    "        'passenger_count': 'Int64', 'fare_amount': 'float64',\n",
    "        'extra': 'float64', 'mta_tax': 'float64', 'tip_amount': 'float64',\n",
    "        'tolls_amount': 'float64', 'improvement_surcharge': 'float64',\n",
    "        'total_amount': 'float64', 'payment_type': 'Int64',\n",
    "        'congestion_surcharge': 'float64'\n",
    "    }\n",
    "    df = pd.read_csv(os.path.join(RAW_DIR, file), dtype=dtypes)\n",
    "    taxi_dfs.append(df)\n",
    "\n",
    "taxi_df = pd.concat(taxi_dfs, ignore_index=True)\n",
    "print(f\"Loaded {len(taxi_df):,} taxi trips from {len(taxi_files)} files.\")\n",
    "\n",
    "# Load Weather Data\n",
    "weather_df = pd.read_csv(os.path.join(RAW_DIR, \"weather_2023_Q1.csv\"))\n",
    "print(f\"Loaded {len(weather_df)} weather records.\")\n",
    "\n",
    "# OSM Data (For now, we'll skip the complex Geo-Join. It's often slow for full data.\n",
    "# We'll rely on the existing Location IDs for zone/borough info later.)\n",
    "# osm_gdf = gpd.read_file(os.path.join(RAW_DIR, \"nyc_osm.geojson\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c8e4e8f1-f051-41db-ab89-c0eb3ae3f646",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning and transforming taxi data...\n",
      "Filtered out 611,534 invalid trips. Remaining: 8,772,953\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 2️⃣ Clean Taxi Data\n",
    "# -----------------------------\n",
    "print(\"Cleaning and transforming taxi data...\")\n",
    "\n",
    "# 2.1 Convert Timestamps\n",
    "# Ensure correct datetime parsing\n",
    "taxi_df['tpep_pickup_datetime'] = pd.to_datetime(taxi_df['tpep_pickup_datetime'])\n",
    "taxi_df['tpep_dropoff_datetime'] = pd.to_datetime(taxi_df['tpep_dropoff_datetime'])\n",
    "\n",
    "# Extract Date for joining with weather\n",
    "taxi_df['pickup_date'] = taxi_df['tpep_pickup_datetime'].dt.date\n",
    "taxi_df['pickup_hour'] = taxi_df['tpep_pickup_datetime'].dt.hour\n",
    "taxi_df['pickup_dayofweek'] = taxi_df['tpep_pickup_datetime'].dt.dayofweek # Monday=0, Sunday=6\n",
    "\n",
    "# 2.2 Feature Engineering (Trip Duration)\n",
    "taxi_df['trip_duration'] = (taxi_df['tpep_dropoff_datetime'] - taxi_df['tpep_pickup_datetime']).dt.total_seconds() / 60 # Duration in minutes\n",
    "\n",
    "# 2.3 Filter Invalid/Outlier Trips (Critical Cleaning)\n",
    "initial_count = len(taxi_df)\n",
    "\n",
    "# a) Remove records with zero or negative duration (impossible trips)\n",
    "taxi_df = taxi_df[taxi_df['trip_duration'] > 0]\n",
    "# b) Remove trips lasting over 3 hours (180 mins) or under 1 minute (likely errors)\n",
    "taxi_df = taxi_df[(taxi_df['trip_duration'] <= 180) & (taxi_df['trip_duration'] >= 1)]\n",
    "# c) Remove trips with zero or negative fares (impossible business logic)\n",
    "taxi_df = taxi_df[taxi_df['total_amount'] > 0]\n",
    "# d) Remove trips with zero passenger count\n",
    "taxi_df = taxi_df[taxi_df['passenger_count'] > 0]\n",
    "# e) Remove trips with zero or negative distance\n",
    "taxi_df = taxi_df[taxi_df['trip_distance'] > 0] \n",
    "\n",
    "# Convert 'PULocationID' and 'DOLocationID' to string for consistency in the DB\n",
    "taxi_df['PULocationID'] = taxi_df['PULocationID'].astype(str)\n",
    "taxi_df['DOLocationID'] = taxi_df['DOLocationID'].astype(str)\n",
    "\n",
    "print(f\"Filtered out {initial_count - len(taxi_df):,} invalid trips. Remaining: {len(taxi_df):,}\")\n",
    "\n",
    "# Drop unnecessary/redundant columns before merge\n",
    "cols_to_drop = ['VendorID', 'RatecodeID', 'store_and_fwd_flag', 'extra',\n",
    "                'mta_tax', 'tolls_amount', 'improvement_surcharge','Airport_fee']\n",
    "taxi_df = taxi_df.drop(columns=[col for col in cols_to_drop if col in taxi_df.columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e7bd45e8-9137-402f-9c92-fba3afdd17ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning and transforming weather data...\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "# # -----------------------------\n",
    "# 3️⃣ Clean Weather Data\n",
    "# -----------------------------\n",
    "print(\"Cleaning and transforming weather data...\")\n",
    "\n",
    "# Select relevant columns: DATE, TMAX (Max Temp), TMIN (Min Temp), PRCP (Precipitation)\n",
    "weather_cols = ['DATE', 'TMAX', 'TMIN', 'PRCP']\n",
    "weather_df = weather_df[weather_cols]\n",
    "\n",
    "# 3.1 Convert Date to correct format for joining\n",
    "weather_df['DATE'] = pd.to_datetime(weather_df['DATE']).dt.date\n",
    "\n",
    "# 3.2 Convert units (NOAA data is often in tenths of units)\n",
    "# Temperature is in tenths of a degree C/F. Assuming F in NYC for simplicity, divide by 10.\n",
    "# We'll keep it general (Tenths of Units -> Units)\n",
    "weather_df['TMAX'] = weather_df['TMAX'] / 10\n",
    "weather_df['TMIN'] = weather_df['TMIN'] / 10\n",
    "\n",
    "# Precipitation (PRCP) is in tenths of mm/inches. We'll divide by 10.\n",
    "# Note: For NYC Central Park, it's usually in tenths of mm.\n",
    "weather_df['PRCP'] = weather_df['PRCP'] / 10\n",
    "\n",
    "# 3.3 Create a 'Rain_Day' flag\n",
    "# Assuming a 'rain day' has measurable precipitation (> 0 units)\n",
    "weather_df['Rain_Day'] = np.where(weather_df['PRCP'] > 0, 1, 0)\n",
    "\n",
    "# Rename DATE for merge consistency\n",
    "weather_df = weather_df.rename(columns={'DATE': 'pickup_date',\n",
    "                                        'TMAX': 'max_temp',\n",
    "                                        'TMIN': 'min_temp',\n",
    "                                        'PRCP': 'precipitation'})\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "93407e89-b741-434a-ada4-118d29808f3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging taxi and weather data...\n",
      "Final merged dataset size: 8,772,953\n",
      "Final columns: ['tpep_pickup_datetime', 'tpep_dropoff_datetime', 'passenger_count', 'trip_distance', 'PULocationID', 'DOLocationID', 'payment_type', 'fare_amount', 'tip_amount', 'total_amount', 'congestion_surcharge', 'airport_fee', 'pickup_date', 'pickup_hour', 'pickup_dayofweek', 'trip_duration', 'max_temp', 'min_temp', 'precipitation', 'Rain_Day']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 4️⃣ Merge Taxi and Weather Data\n",
    "# -----------------------------\n",
    "print(\"Merging taxi and weather data...\")\n",
    "\n",
    "# Perform an inner join to only keep taxi trips that have matching weather data\n",
    "final_df = pd.merge(\n",
    "    taxi_df,\n",
    "    weather_df,\n",
    "    on='pickup_date',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Convert pickup_date back to datetime for PostgreSQL consistency and easy querying\n",
    "final_df['pickup_date'] = pd.to_datetime(final_df['pickup_date'])\n",
    "\n",
    "print(f\"Final merged dataset size: {len(final_df):,}\")\n",
    "print(\"Final columns:\", final_df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "92e53698-4950-4dd7-a57a-43803ab1ec29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Cleaned and transformed data saved to: ../data_clean\\cleaned_nyc_trips_q1.csv\n",
      "Ready for Step 7: Load Clean Data to PostgreSQL.\n"
     ]
    }
   ],
   "source": [
    "# # -----------------------------\n",
    "# 5️⃣ Save Final Clean Data\n",
    "# -----------------------------\n",
    "output_path = os.path.join(CLEAN_DIR, \"cleaned_nyc_trips_q1.csv\")\n",
    "final_df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"\\n✅ Cleaned and transformed data saved to: {output_path}\")\n",
    "print(\"Ready for Step 7: Load Clean Data to PostgreSQL.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bd7079a4-a86a-4fa8-9b60-2d5e81718d0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Merged dataset shape: (9177624, 17)\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15423746-0c7a-45f3-bb8f-e866cd2f52f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbcea9fe-68be-4f54-8c94-bf2480cbaa7f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
