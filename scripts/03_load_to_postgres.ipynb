{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f46ced42-207e-45f4-b7b9-3f002f810b0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Successfully created database engine.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, text\n",
    "import os\n",
    "\n",
    "# --- Configuration ---\n",
    "CLEAN_DIR = \"../data_clean\"\n",
    "DATA_FILE = \"cleaned_nyc_trips_q1.csv\"\n",
    "TABLE_NAME = \"taxi_trips\"\n",
    "\n",
    "# PostgreSQL Connection Details (Customize these!)\n",
    "DB_USER = \"postgres\"  # Your PostgreSQL username\n",
    "DB_PASS = \"etes1209111\" # !! Replace with your actual password !!\n",
    "# DB_HOST = \"localhost\"\n",
    "DB_HOST = \"127.0.0.1\" \n",
    "DB_PORT = \"5432\"\n",
    "DB_NAME = \"nyc_taxi\"\n",
    "\n",
    "# SQLAlchemy Connection String\n",
    "DATABASE_URL = f\"postgresql://{DB_USER}:{DB_PASS}@{DB_HOST}:{DB_PORT}/{DB_NAME}\"\n",
    "\n",
    "# Create the SQLAlchemy engine\n",
    "try:\n",
    "    engine = create_engine(DATABASE_URL)\n",
    "    print(\"‚úÖ Successfully created database engine.\")\n",
    "except ImportError:\n",
    "    print(\"‚ùå ERROR: Ensure psycopg2-binary is installed: pip install psycopg2-binary\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e27d204-7d0f-45b3-ac14-ee22846d8121",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating table 'taxi_trips' in PostgreSQL...\n",
      "‚úÖ Table 'taxi_trips' created successfully.\n"
     ]
    }
   ],
   "source": [
    "# --- Define Schema for taxi_trips Table ---\n",
    "# We define this as a transaction in Python to execute it cleanly.\n",
    "\n",
    "# Note: We use VARCHAR for Location IDs and payment types initially\n",
    "# to ensure compatibility, though INTEGER is also possible.\n",
    "# Double precision is used for floats for accuracy.\n",
    "\n",
    "create_table_sql = f\"\"\"\n",
    "DROP TABLE IF EXISTS {TABLE_NAME};\n",
    "CREATE TABLE {TABLE_NAME} (\n",
    "    tpep_pickup_datetime TIMESTAMP WITHOUT TIME ZONE,\n",
    "    tpep_dropoff_datetime TIMESTAMP WITHOUT TIME ZONE,\n",
    "    passenger_count INTEGER,\n",
    "    trip_distance DOUBLE PRECISION,\n",
    "    pulocationid VARCHAR(10),     -- All lowercase\n",
    "    dolocationid VARCHAR(10),     -- All lowercase\n",
    "    payment_type INTEGER,\n",
    "    fare_amount DOUBLE PRECISION,\n",
    "    tip_amount DOUBLE PRECISION,\n",
    "    total_amount DOUBLE PRECISION,\n",
    "    congestion_surcharge DOUBLE PRECISION,\n",
    "    airport_fee DOUBLE PRECISION, -- The one remaining airport_fee column\n",
    "    pickup_date DATE,\n",
    "    pickup_hour INTEGER,\n",
    "    pickup_dayofweek INTEGER,\n",
    "    trip_duration DOUBLE PRECISION,\n",
    "    max_temp DOUBLE PRECISION,\n",
    "    min_temp DOUBLE PRECISION,\n",
    "    precipitation DOUBLE PRECISION,\n",
    "    rain_day INTEGER              -- All lowercase\n",
    ");\n",
    "\"\"\"\n",
    "# ‚û°Ô∏è Rerun the block that executes this SQL to drop and recreate the table.\n",
    "\n",
    "print(f\"Creating table '{TABLE_NAME}' in PostgreSQL...\")\n",
    "with engine.connect() as connection:\n",
    "    connection.execute(text(create_table_sql))\n",
    "    connection.commit()\n",
    "print(f\"‚úÖ Table '{TABLE_NAME}' created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1969687a-d50c-4991-a184-1dc8ad6afc75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting chunked data load from cleaned_nyc_trips_q1.csv to PostgreSQL.\n",
      "Processing chunk #1...\n",
      "‚úÖ Chunk #1 loaded successfully.\n",
      "Processing chunk #2...\n",
      "‚úÖ Chunk #2 loaded successfully.\n",
      "Processing chunk #3...\n",
      "‚úÖ Chunk #3 loaded successfully.\n",
      "Processing chunk #4...\n",
      "‚úÖ Chunk #4 loaded successfully.\n",
      "Processing chunk #5...\n",
      "‚úÖ Chunk #5 loaded successfully.\n",
      "Processing chunk #6...\n",
      "‚úÖ Chunk #6 loaded successfully.\n",
      "Processing chunk #7...\n",
      "‚úÖ Chunk #7 loaded successfully.\n",
      "Processing chunk #8...\n",
      "‚úÖ Chunk #8 loaded successfully.\n",
      "Processing chunk #9...\n",
      "‚úÖ Chunk #9 loaded successfully.\n",
      "Processing chunk #10...\n",
      "‚úÖ Chunk #10 loaded successfully.\n",
      "Processing chunk #11...\n",
      "‚úÖ Chunk #11 loaded successfully.\n",
      "Processing chunk #12...\n",
      "‚úÖ Chunk #12 loaded successfully.\n",
      "Processing chunk #13...\n",
      "‚úÖ Chunk #13 loaded successfully.\n",
      "Processing chunk #14...\n",
      "‚úÖ Chunk #14 loaded successfully.\n",
      "Processing chunk #15...\n",
      "‚úÖ Chunk #15 loaded successfully.\n",
      "Processing chunk #16...\n",
      "‚úÖ Chunk #16 loaded successfully.\n",
      "Processing chunk #17...\n",
      "‚úÖ Chunk #17 loaded successfully.\n",
      "Processing chunk #18...\n",
      "‚úÖ Chunk #18 loaded successfully.\n",
      "\n",
      "üéâ Chunked data load complete. Total chunks loaded: 18.\n"
     ]
    }
   ],
   "source": [
    "# --- Load Data from CSV IN CHUNKS ---\n",
    "file_path = os.path.join(CLEAN_DIR, DATA_FILE)\n",
    "if not os.path.exists(file_path):\n",
    "    print(f\"‚ùå ERROR: Clean data file not found at: {file_path}\")\n",
    "    exit()\n",
    "\n",
    "print(f\"Starting chunked data load from {DATA_FILE} to PostgreSQL.\")\n",
    "\n",
    "# Define chunk size (adjust this based on your available memory)\n",
    "# 500,000 rows is a good starting point for chunking\n",
    "CHUNK_SIZE = 500000 \n",
    "chunk_num = 0\n",
    "\n",
    "try:\n",
    "    # Use pandas.read_csv with 'iterator=True' or 'chunksize'\n",
    "    for chunk_df in pd.read_csv(file_path, chunksize=CHUNK_SIZE, low_memory=False):\n",
    "        \n",
    "        print(f\"Processing chunk #{chunk_num + 1}...\")\n",
    "\n",
    "        # 1. FIX: Convert all DataFrame column names to lowercase to match the SQL schema\n",
    "        chunk_df.columns = chunk_df.columns.str.lower()\n",
    "        \n",
    "        # 2. Convert DATE and TIMESTAMP columns (using new lowercase names)\n",
    "        chunk_df['tpep_pickup_datetime'] = pd.to_datetime(chunk_df['tpep_pickup_datetime'])\n",
    "        chunk_df['tpep_dropoff_datetime'] = pd.to_datetime(chunk_df['tpep_dropoff_datetime'])\n",
    "        chunk_df['pickup_date'] = pd.to_datetime(chunk_df['pickup_date']) \n",
    "        \n",
    "        # 3. Handle the duplicate 'airport_fee' (if still present in the CSV)\n",
    "        # Assuming the duplicate column was dropped in the cleaning script, \n",
    "        # this check can often be skipped, but keep it in mind.\n",
    "        \n",
    "        # --- Push Chunk to PostgreSQL ---\n",
    "        # 'if_exists='append' is used to add data to the table created earlier\n",
    "        chunk_df.to_sql(TABLE_NAME, engine, if_exists='append', index=False)\n",
    "        \n",
    "        print(f\"‚úÖ Chunk #{chunk_num + 1} loaded successfully.\")\n",
    "        chunk_num += 1\n",
    "\n",
    "    print(f\"\\nüéâ Chunked data load complete. Total chunks loaded: {chunk_num}.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå ERROR during chunked data load: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2a51881-a6b9-49da-9cdd-d1fac31ca073",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting data validation checks...\n",
      "‚úÖ Total rows loaded in PostgreSQL table 'taxi_trips': 8,772,953\n",
      "\n",
      "‚úÖ First 5 rows of data:\n",
      "  tpep_pickup_datetime tpep_dropoff_datetime  passenger_count  trip_distance  \\\n",
      "0  2023-01-01 00:32:10   2023-01-01 00:40:36                1           0.97   \n",
      "1  2023-01-01 00:55:08   2023-01-01 01:01:27                1           1.10   \n",
      "2  2023-01-01 00:25:04   2023-01-01 00:37:49                1           2.51   \n",
      "3  2023-01-01 00:10:29   2023-01-01 00:21:19                1           1.43   \n",
      "4  2023-01-01 00:50:34   2023-01-01 01:02:52                1           1.84   \n",
      "\n",
      "  pulocationid dolocationid  payment_type  fare_amount  tip_amount  \\\n",
      "0          161          141             2          9.3        0.00   \n",
      "1           43          237             1          7.9        4.00   \n",
      "2           48          238             1         14.9       15.00   \n",
      "3          107           79             1         11.4        3.28   \n",
      "4          161          137             1         12.8       10.00   \n",
      "\n",
      "   total_amount  congestion_surcharge  airport_fee pickup_date  pickup_hour  \\\n",
      "0         14.30                   2.5          0.0  2023-01-01            0   \n",
      "1         16.90                   2.5          0.0  2023-01-01            0   \n",
      "2         34.90                   2.5          0.0  2023-01-01            0   \n",
      "3         19.68                   2.5          0.0  2023-01-01            0   \n",
      "4         27.80                   2.5          0.0  2023-01-01            0   \n",
      "\n",
      "   pickup_dayofweek  trip_duration  max_temp  min_temp  precipitation  \\\n",
      "0                 6       8.433333      12.8       9.4            0.0   \n",
      "1                 6       6.316667      12.8       9.4            0.0   \n",
      "2                 6      12.750000      12.8       9.4            0.0   \n",
      "3                 6      10.833333      12.8       9.4            0.0   \n",
      "4                 6      12.300000      12.8       9.4            0.0   \n",
      "\n",
      "   rain_day  \n",
      "0         0  \n",
      "1         0  \n",
      "2         0  \n",
      "3         0  \n",
      "4         0  \n",
      "\n",
      "‚úÖ Column names and data types (PostgreSQL inferred):\n",
      "tpep_pickup_datetime     datetime64[ns]\n",
      "tpep_dropoff_datetime    datetime64[ns]\n",
      "passenger_count                   int64\n",
      "trip_distance                   float64\n",
      "pulocationid                     object\n",
      "dolocationid                     object\n",
      "payment_type                      int64\n",
      "fare_amount                     float64\n",
      "tip_amount                      float64\n",
      "total_amount                    float64\n",
      "congestion_surcharge            float64\n",
      "airport_fee                     float64\n",
      "pickup_date                      object\n",
      "pickup_hour                       int64\n",
      "pickup_dayofweek                  int64\n",
      "trip_duration                   float64\n",
      "max_temp                        float64\n",
      "min_temp                        float64\n",
      "precipitation                   float64\n",
      "rain_day                          int64\n",
      "dtype: object\n",
      "\n",
      "‚úÖ Invalid trips (distance <= 0): 0\n"
     ]
    }
   ],
   "source": [
    "engine = create_engine(DATABASE_URL)\n",
    "\n",
    "print(\"Starting data validation checks...\")\n",
    "\n",
    "try:\n",
    "    with engine.connect() as connection:\n",
    "        \n",
    "        # 1. Row Count Check\n",
    "        # Get the total number of rows in the loaded table\n",
    "        count_query = text(f\"SELECT COUNT(*) FROM {TABLE_NAME};\")\n",
    "        row_count = connection.execute(count_query).scalar()\n",
    "        print(f\"‚úÖ Total rows loaded in PostgreSQL table '{TABLE_NAME}': {row_count:,}\")\n",
    "\n",
    "        # Note: You should compare this count to the number of rows in your final_df \n",
    "        # from the cleaning script (it was printed as 'Final merged dataset size').\n",
    "        \n",
    "        # 2. Schema Check\n",
    "        # Display the first 5 rows and column names/types to confirm schema integrity\n",
    "        sample_query = text(f\"SELECT * FROM {TABLE_NAME} LIMIT 5;\")\n",
    "        sample_df = pd.read_sql(sample_query, connection)\n",
    "        \n",
    "        print(\"\\n‚úÖ First 5 rows of data:\")\n",
    "        print(sample_df)\n",
    "        \n",
    "        print(\"\\n‚úÖ Column names and data types (PostgreSQL inferred):\")\n",
    "        print(sample_df.dtypes)\n",
    "        \n",
    "        # 3. Basic Data Integrity Check (Example: Check for negative trip distances)\n",
    "        integrity_query = text(f\"SELECT COUNT(*) FROM {TABLE_NAME} WHERE trip_distance <= 0;\")\n",
    "        invalid_trips = connection.execute(integrity_query).scalar()\n",
    "        \n",
    "        print(f\"\\n‚úÖ Invalid trips (distance <= 0): {invalid_trips:,}\")\n",
    "        # This count should ideally be 0, as you filtered these in the cleaning step.\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå ERROR during data validation: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d2ce26-7e3d-486c-b26a-d37539189e4e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
